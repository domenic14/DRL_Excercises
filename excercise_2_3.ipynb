{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 2.3 - CartPole Umgebung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Einfache Visualisierung mit zufällig ausgewählten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "\n",
    "visualization_iterations = 10\n",
    "\n",
    "for _ in range(visualization_iterations):\n",
    "    observation, info = env.reset()\n",
    "    \n",
    "    episode_over = False\n",
    "    \n",
    "    while not episode_over:\n",
    "        action = env.action_space.sample()  # random right or left\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        episode_over = terminated or truncated\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) - Implementierung eines intuitiven Lösungsansatzes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = 100\n",
    "num_episodes = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy 1: Wenn der Pole nach links geneigt ist, fahre nach links. Wenn der Pole nach rechts geneigt ist, fahre nach rechts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, info = env.reset()\n",
    "\n",
    "rewards = np.zeros(num_episodes)\n",
    "\n",
    "for _ in range(runs):\n",
    "    \n",
    "    for episode_index in range(num_episodes):\n",
    "\n",
    "        observation, info = env.reset()\n",
    "\n",
    "        episode_over = False\n",
    "        episode_reward = 0\n",
    "\n",
    "        while not episode_over:\n",
    "            action = 1  # push cart to the right\n",
    "            \n",
    "            # if the pole is leaning to the left, push the cart to the left\n",
    "            if observation[2] < 0:\n",
    "                action = 0\n",
    "            \n",
    "            observation, reward, terminated, truncated, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "\n",
    "            episode_over = terminated or truncated\n",
    "        \n",
    "        rewards[episode_index] += episode_reward\n",
    "\n",
    "rewards_p1 = np.divide(rewards, runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy 2: Wenn der Pole nach links geneigt ist aber eine positive Winkelgeschwindigkeit hat, fahre wieder nach rechts. Analog für Neigung nach rechts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, info = env.reset()\n",
    "\n",
    "rewards = np.zeros(num_episodes)\n",
    "\n",
    "for _ in range(runs):\n",
    "    \n",
    "    for episode_index in range(num_episodes):\n",
    "\n",
    "        observation, info = env.reset()\n",
    "\n",
    "        episode_over = False\n",
    "        episode_reward = 0\n",
    "\n",
    "        while not episode_over:\n",
    "            action = 1  # push cart to the right\n",
    "            \n",
    "            # if the pole is leaning to the left, push the cart to the left\n",
    "            falling_left = observation[3] < 0\n",
    "            if falling_left:\n",
    "                action = 0\n",
    "            \n",
    "            observation, reward, terminated, truncated, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "\n",
    "            episode_over = terminated or truncated\n",
    "        \n",
    "        rewards[episode_index] += episode_reward\n",
    "\n",
    "rewards_p2 = np.divide(rewards, runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy 3: Zusätzliche Vermeidung des \"Herausfahrens\" aus der Umgebung:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, info = env.reset()\n",
    "\n",
    "rewards = np.zeros(num_episodes)\n",
    "\n",
    "for _ in range(runs):\n",
    "\n",
    "    for episode_index in range(num_episodes):\n",
    "\n",
    "        observation, info = env.reset()\n",
    "\n",
    "        episode_over = False\n",
    "        episode_reward = 0\n",
    "\n",
    "        while not episode_over:\n",
    "            action = 1  # push cart to the right\n",
    "            \n",
    "            # if the pole is falling to the left, push the cart to the left\n",
    "            # (falling meaning the angular velocity is negative)\n",
    "            falling_left = observation[3] < 0\n",
    "            if falling_left:\n",
    "                action = 0\n",
    "            \n",
    "            # do not drive out of the environment\n",
    "            cart_position = observation[0]\n",
    "            if cart_position <= -2.2:\n",
    "                action = 1\n",
    "            elif cart_position >= 2.2:\n",
    "                action = 0\n",
    "\n",
    "            observation, reward, terminated, truncated, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "\n",
    "            episode_over = terminated or truncated\n",
    "\n",
    "        rewards[episode_index] += episode_reward \n",
    "\n",
    "rewards_p3 = np.divide(rewards, runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment schließen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auswerten der Policy-Ergebnisse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, num_episodes, num_episodes)\n",
    "\n",
    "plt.plot(x, rewards_p1, label = 'Policy 1')\n",
    "plt.plot(x, rewards_p2, label = 'Policy 2')\n",
    "plt.plot(x, rewards_p3, label = 'Policy 3')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Reward per Episode')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) - RL-Ansatz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ObservationWrapper erstellen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DiscreteObservationWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(DiscreteObservationWrapper, self).__init__(env)\n",
    "        self.pole_pos_bins = np.array([-np.inf, 0.0, np.inf])\n",
    "        self.pole_vel_bins = np.array([-np.inf, 0.0, np.inf])\n",
    "        self.num_states = 4\n",
    "        \n",
    "        self.min_state_idx = self.get_state_index(1, 1) # since the values are always greater than negative infinity, they are always in category 1 and never in zero. Thats why category tuple (1, 1) leads to the lowest individual state index\n",
    "\n",
    "    def get_state_index(self, pole_pos_category, pole_vel_category):\n",
    "        return 2 * pole_pos_category + pole_vel_category\n",
    "\n",
    "    def observation(self, observation):\n",
    "        _, _, pole_pos, pole_vel = observation\n",
    "        \n",
    "        pole_pos_category = np.digitize(pole_pos, self.pole_pos_bins)\n",
    "        pole_vel_category = np.digitize(pole_vel, self.pole_vel_bins)\n",
    "\n",
    "        # basically binary \"or\" operation, each category gets its own bit\n",
    "        state_index = self.get_state_index(pole_pos_category, pole_vel_category)\n",
    "\n",
    "        return state_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Den Banditen durch eine Klasse modellieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class StateBandit():\n",
    "    def __init__(self, epsilon, initial_Q = 0.0):\n",
    "        self.epsilon = epsilon\n",
    "        self.Q = np.array([initial_Q, initial_Q])\n",
    "        self.N = np.array([0, 0])\n",
    "        self.G = 0\n",
    "        self.reward_per_action = []\n",
    "\n",
    "    def get_action(self):\n",
    "        action = np.argmax(self.Q)\n",
    "\n",
    "        # decide on exploit vs explore\n",
    "        explore = random.random() <= self.epsilon\n",
    "        if explore:\n",
    "            action = int(random.random() >= 0.5)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def update_metrics(self, action, reward):\n",
    "        self.reward_per_action.append(reward)\n",
    "        self.G += reward\n",
    "        self.N[action] += 1\n",
    "        self.Q[action] = self.Q[action] + ((reward - self.Q[action]) / self.N[action])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für jeden State einen Banditen erstellen und die Simulation laufen lassen, um die Banditen anzulernen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import stdout # flushing stdout for progress printing\n",
    "\n",
    "# create env with wrapper\n",
    "env = gym.make('CartPole-v1')\n",
    "wrapped_env = DiscreteObservationWrapper(env)\n",
    "\n",
    "# setting up the bandits\n",
    "epsilon_value = 0.1\n",
    "initial_Q = 1.0\n",
    "bandits = [StateBandit(epsilon_value, initial_Q) for _ in range(wrapped_env.num_states)]\n",
    "\n",
    "# training parameters\n",
    "simulation_steps = 40_000\n",
    "\n",
    "# reset wrapped env\n",
    "state_index, _ = wrapped_env.reset()\n",
    "episode_over = False\n",
    "\n",
    "# training loop\n",
    "progress = -1\n",
    "sim_step = 0\n",
    "while sim_step < simulation_steps:\n",
    "    \n",
    "    new_progress = int(((sim_step + 1) * 100) / simulation_steps)\n",
    "    if new_progress > progress:\n",
    "        progress = new_progress\n",
    "        print(f'\\rProgress: {progress} %', end='')\n",
    "        stdout.flush()\n",
    "\n",
    "    bandit_index = state_index - wrapped_env.min_state_idx\n",
    "    action = bandits[bandit_index].get_action()\n",
    "    state_index, reward, terminated, truncated, _ = wrapped_env.step(action)\n",
    "\n",
    "    reward = -10 if terminated else reward\n",
    "\n",
    "    bandits[bandit_index].update_metrics(action, reward)\n",
    "\n",
    "    episode_over = terminated or truncated\n",
    "    \n",
    "    if episode_over:\n",
    "        sim_step += 1\n",
    "        episode_over = False\n",
    "        state_index, _ = wrapped_env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotten des Lernprozesses, Nutzung eines \"Dictionarys\", Banditen-Index auf die Zustandsbeschreibung zu mappen. Die Verwendung eines vollwertigen Python-Dicts ist hier nicht notwendig, da die Banditen von Index 0 aufsteigend geplottet werden. Das heißt, man kann die Zustandsbeschreibungen auch chronologisch ablegen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_index_description = ['Negative angle, negative velocity', 'Negative angle, positive velocity', 'Positive angle, Negative velocity', 'Positive angle, Positive velocity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(len(bandits), figsize=(15, 15))\n",
    "for bandit_index in range(len(bandits)):\n",
    "    bandit = bandits[bandit_index]\n",
    "    ax[bandit_index].plot(bandit.reward_per_action, label = f'State {bandit_index} ({state_index_description[bandit_index]})')\n",
    "    ax[bandit_index].set_xlabel('# Actions')\n",
    "    ax[bandit_index].set_ylabel('Reward per Action')\n",
    "    ax[bandit_index].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testen, wie gut die Banditen sind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 200\n",
    "runs = 100\n",
    "\n",
    "state_index, info = wrapped_env.reset()\n",
    "\n",
    "rewards = np.zeros(num_episodes)\n",
    "\n",
    "for _ in range(runs):\n",
    "    \n",
    "    for episode_index in range(num_episodes):\n",
    "\n",
    "        state_index, info = wrapped_env.reset()\n",
    "\n",
    "        episode_over = False\n",
    "        episode_reward = 0\n",
    "\n",
    "        while not episode_over:\n",
    "            bandit_index = state_index - wrapped_env.min_state_idx\n",
    "            action = bandits[bandit_index].get_action()\n",
    "\n",
    "            state_index, reward, terminated, truncated, info = wrapped_env.step(action)\n",
    "            episode_reward += reward\n",
    "\n",
    "            episode_over = terminated or truncated\n",
    "        \n",
    "        rewards[episode_index] += episode_reward\n",
    "\n",
    "rewards_RL = np.divide(rewards, runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Darstellung der Testergebnisse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, num_episodes, num_episodes)\n",
    "\n",
    "plt.plot(x, rewards_RL, label = 'Reinforcement Learning')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Reward per Episode')\n",
    "plt.ylim(0, 240)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schließen der Environments (Da diese noch für den Test verwendet wurden, kann man sie nicht direkt nach dem Training schließen):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_env.close()\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_reinforcement",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
