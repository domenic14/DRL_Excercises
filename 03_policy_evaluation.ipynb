{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from enum import Enum\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grid is represented by the \"GridWorld\" class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Direction(Enum):\n",
    "    RIGHT = (0, 1)\n",
    "    LEFT = (0, -1)\n",
    "    UP = (-1, 0)\n",
    "    DOWN = (1, 0)\n",
    "\n",
    "\n",
    "class GridWorld:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.state_dtype = np.dtype([('key', 'U2'), ('state_value', 'f4')])\n",
    "        self.grid = np.array([[(None, 0) for _ in range(5)]\n",
    "                             for _ in range(5)], dtype=self.state_dtype)\n",
    "        self.grid[0, 1]['key'] = 'A'\n",
    "        self.grid[0, 3]['key'] = 'B'\n",
    "        self.grid[4, 1]['key'] = 'A_'\n",
    "        self.grid[2, 3]['key'] = 'B_'\n",
    "\n",
    "    def move(self, position, direction: Direction):\n",
    "        # basically (pos[0]+dir[0], pos[1]+dir[1])\n",
    "        new_position = tuple(sum(x) for x in zip(position, direction.value))\n",
    "\n",
    "        if self.grid[position]['key'] == 'A':\n",
    "            new_position = (4,1)\n",
    "            return 10, new_position\n",
    "\n",
    "        elif self.grid[position]['key'] == 'B':\n",
    "            new_position = (2,3)\n",
    "            return 5, new_position\n",
    "        \n",
    "        if new_position[0] < 0 or new_position[1] < 0 or new_position[0] > 4 or new_position[1] > 4:\n",
    "            return -1, position\n",
    "        else:\n",
    "            return 0, new_position\n",
    "\n",
    "    def get_state_value_after_move(self, position, direction):\n",
    "        '''\n",
    "        Gets the state value of the cell which the player would be on after moving in the provided direction. The function does not actually move the player and thus does not return a new position.\n",
    "        '''\n",
    "\n",
    "        if self.grid[position]['key'] == 'A':\n",
    "            return self.grid[(4, 1)]['state_value']\n",
    "        \n",
    "        if self.grid[position]['key'] == 'B':\n",
    "            return self.grid[(2, 3)]['state_value']\n",
    "        \n",
    "        new_position = tuple(sum(x) for x in zip(position, direction.value))\n",
    "\n",
    "        if new_position[0] < 0 or new_position[1] < 0 or new_position[0] > 4 or new_position[1] > 4:\n",
    "            return -np.inf\n",
    "        \n",
    "        return self.grid[new_position]['state_value']\n",
    "\n",
    "    def reset(self):\n",
    "        self.grid = np.array([[(None, 0) for _ in range(5)] for _ in range(5)], dtype=self.state_dtype)\n",
    "        self.grid[0, 1]['key'] = 'A'\n",
    "        self.grid[0, 3]['key'] = 'B'\n",
    "        self.grid[4, 1]['key'] = 'A_'\n",
    "        self.grid[2, 3]['key'] = 'B_'\n",
    "\n",
    "    def __str__(self):\n",
    "        print_string = '--- GRID ---\\n'\n",
    "        for row in self.grid:\n",
    "            print_string += '- ' * len(self.grid[0]) * 5\n",
    "            print_string += '\\n'\n",
    "\n",
    "            print_string += '|'\n",
    "            for cell in row:\n",
    "                cell_value = cell['state_value']\n",
    "\n",
    "                # create equal spacing for printing    \n",
    "                value_prefix = ' ' if cell_value > 0 else ''\n",
    "                value_prefix = '' if cell_value >= 10 else value_prefix\n",
    "                cell_value = round(cell_value, 4)\n",
    "                print_string += f' {value_prefix}{cell_value:.4f} |'\n",
    "            \n",
    "            print_string += '\\n'\n",
    "        print_string += '- ' * len(self.grid[0]) * 5\n",
    "        print_string += '\\n--- END GRID ---\\n'\n",
    "\n",
    "        return print_string\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the class which represents the policy for each cell. Basically holds an array for the possible actions for each cell. If there are more than one action, each one has the same probability of being taken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CellPolicy:\n",
    "    \n",
    "    def __init__(self, initial_choices):\n",
    "        self.choices = np.array(initial_choices)\n",
    "\n",
    "    def update(self, new_choices):\n",
    "        self.choices = np.array(new_choices) \n",
    "\n",
    "    def get_choices(self) -> np.ndarray:\n",
    "        return self.choices\n",
    "    \n",
    "    def __str__(self):\n",
    "        choices_to_sign_map = [None, # cannot happen would mean that no action is possible\n",
    "                               '🡢', # left\n",
    "                               '🡠', # right\n",
    "                               '⮂', # left + right\n",
    "                               '🡡', # up\n",
    "                               '🡥', # up + right\n",
    "                               '🡤', # up + left\n",
    "                               '⤧', # up + right + left\n",
    "                               '🡣', # down\n",
    "                               '🡦', # down + right\n",
    "                               '🡧', # down + left\n",
    "                               '⤩', # down + right + left\n",
    "                               '⮁', # up + down\n",
    "                               '⤨', # right + up + down\n",
    "                               '⤪', # left + up + down\n",
    "                               '⬤'] # all directionsS\n",
    "        \n",
    "        enum_val_map = {\n",
    "            Direction.RIGHT : 0,\n",
    "            Direction.LEFT : 1,\n",
    "            Direction.UP : 2,\n",
    "            Direction.DOWN : 3\n",
    "        }\n",
    "\n",
    "        mapped_directions = np.array([enum_val_map[direction] for direction in self.choices])\n",
    "        occurences = np.zeros((4,))\n",
    "        occurences[mapped_directions] = 1\n",
    "\n",
    "        # calculate a unique index [1-15] for each combination of possible actions\n",
    "        index = 0\n",
    "        for i in range(len(occurences)):\n",
    "            index+=2**i * occurences[i]\n",
    "\n",
    "        return choices_to_sign_map[int(index)]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple print function for the policy grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_policy(policy):\n",
    "    print('--- POLICY ---')\n",
    "    for row in policy:\n",
    "        print('- ' * 12)\n",
    "        print('|', end='')\n",
    "        for cell_policy in row:\n",
    "            print(f' {cell_policy} |', end='')\n",
    "        \n",
    "        print()\n",
    "    print('- ' * 12)\n",
    "    print('--- END POLICY ---')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function for updating the policy based on the current grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(policy : np.ndarray, grid_world : GridWorld) -> Tuple[np.ndarray, bool]:\n",
    "    changed_policy = False\n",
    "    for index, cell_policy in np.ndenumerate(policy):\n",
    "        actions = policy[index].get_choices()\n",
    "        corresponding_state_values = np.array([grid_world.get_state_value_after_move(index, action) for action in actions])\n",
    "        max_state_value = np.max(corresponding_state_values)\n",
    "        max_indices = np.where(corresponding_state_values == max_state_value)\n",
    "        corresponding_state_values = corresponding_state_values[max_indices]\n",
    "        if len(corresponding_state_values) < len(actions):\n",
    "            changed_policy = True\n",
    "            cell_policy.update(actions[max_indices])\n",
    "        \n",
    "    return policy, changed_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1 a) - Update the policy only after state value convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 30 iterations for state value convergence.\n",
      "Took 28 iterations for state value convergence.\n",
      "--- POLICY ---\n",
      "- - - - - - - - - - - - \n",
      "| 🡢 | ⬤ | 🡠 | ⬤ | 🡠 |\n",
      "- - - - - - - - - - - - \n",
      "| 🡡 | 🡡 | 🡡 | 🡡 | 🡠 |\n",
      "- - - - - - - - - - - - \n",
      "| 🡡 | 🡡 | 🡡 | 🡡 | 🡡 |\n",
      "- - - - - - - - - - - - \n",
      "| 🡡 | 🡡 | 🡡 | 🡡 | 🡡 |\n",
      "- - - - - - - - - - - - \n",
      "| 🡡 | 🡡 | 🡡 | 🡡 | 🡡 |\n",
      "- - - - - - - - - - - - \n",
      "--- END POLICY ---\n",
      "\n",
      "--- GRID ---\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "| 21.9775 | 24.4194 | 21.9775 | 18.4475 | 16.6028 |\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "| 19.7797 | 21.9775 | 19.7797 | 16.6028 | 14.9425 |\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "| 17.8018 | 19.7797 | 17.8018 | 14.9425 | 13.4483 |\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "| 16.0216 | 17.8018 | 16.0216 | 13.4483 | 12.1034 |\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "| 14.4194 | 16.0216 | 14.4194 | 12.1034 | 10.8931 |\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "--- END GRID ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grid_world = GridWorld()\n",
    "discount = 0.9\n",
    "\n",
    "# initially: random policy\n",
    "policy = np.array([[CellPolicy([Direction.UP, Direction.RIGHT, Direction.DOWN, Direction.LEFT]) for _ in range(5)] for _ in range(5)])\n",
    "\n",
    "changed_policy = True\n",
    "while(changed_policy):\n",
    "\n",
    "    grid_world.reset()\n",
    "    diff = np.inf\n",
    "    \n",
    "    it = 0\n",
    "    # calculate the state values\n",
    "    while(diff > 0.001):\n",
    "        it += 1\n",
    "        diff = 0.0\n",
    "        for index, value in np.ndenumerate(grid_world.grid):\n",
    "            new_value = 0\n",
    "            possible_actions = policy[index].get_choices()\n",
    "            for action in possible_actions:\n",
    "                reward, next_state = grid_world.move(index, action)\n",
    "                new_value += (1 / len(possible_actions)) * (reward + discount *\n",
    "                                        grid_world.grid[next_state]['state_value'])\n",
    "            \n",
    "            new_diff = abs(new_value - value['state_value'])\n",
    "            diff = max(diff, new_diff)\n",
    "\n",
    "            grid_world.grid[index]['state_value'] = new_value\n",
    "\n",
    "\n",
    "    # update the policy    \n",
    "    policy, changed_policy = update_policy(policy, grid_world)  \n",
    "    print(f'Took {it} iterations for state value convergence.')\n",
    "\n",
    "\n",
    "print_policy(policy)\n",
    "print(grid_world)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1 b) - Update the policy after each state value iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_world = GridWorld()\n",
    "discount = 0.9\n",
    "diff_arr = []\n",
    "\n",
    "# initially: random policy\n",
    "policy = np.array([[CellPolicy([Direction.UP, Direction.RIGHT, Direction.DOWN, Direction.LEFT]) for _ in range(5)] for _ in range(5)])\n",
    "\n",
    "diff = np.inf\n",
    "changed_policy = True\n",
    "\n",
    "print('Initial policy:')\n",
    "print_policy(policy)\n",
    "\n",
    "# calculate the state values\n",
    "iteration_count = 0\n",
    "while(diff > 0.001):\n",
    "    iteration_count += 1\n",
    "    diff = 0.0\n",
    "    for index, value in np.ndenumerate(grid_world.grid):\n",
    "        new_value = 0\n",
    "        possible_actions = policy[index].get_choices()\n",
    "        for action in possible_actions:\n",
    "            reward, next_state = grid_world.move(index, action)\n",
    "            new_value += (1 / len(possible_actions)) * (reward + discount *\n",
    "                                    grid_world.grid[next_state]['state_value'])\n",
    "        \n",
    "        new_diff = abs(new_value - value['state_value'])\n",
    "        diff = max(diff, new_diff)\n",
    "\n",
    "        grid_world.grid[index]['state_value'] = new_value\n",
    "\n",
    "    # after each iteration for the state values: update the policy\n",
    "    policy, _ = update_policy(policy, grid_world)  \n",
    "\n",
    "    print(f'--- After iteration {iteration_count} --- ')\n",
    "    print_policy(policy)\n",
    "    print(grid_world)\n",
    "\n",
    "    diff_arr.append(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1 c) - Using the Action Value Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map a direction (action) to a certain index. This becomes important when certain actions are removed from the policy but we still want to map the existing ones to the correct locations in the action value array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_for_action(action : Direction):\n",
    "    dir_index_map = {\n",
    "        Direction.RIGHT : 0,\n",
    "        Direction.LEFT : 1,\n",
    "        Direction.UP : 2,\n",
    "        Direction.DOWN : 3\n",
    "    }\n",
    "\n",
    "    return dir_index_map[action]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function for updating the policy based on the action values.\n",
    "Principle (done for each cell):\n",
    "1. Get the action values for the current cell\n",
    "2. Use the action to index mapping to only get the action values for the remaining actions\n",
    "3. Get the indices of the highest action values\n",
    "4. If necessary: Remove actions from the cell policy which are no longer optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy_av(policy, action_values) -> Tuple[np.ndarray, bool]:\n",
    "    changed_policy = False\n",
    "    for index, cell_policy in np.ndenumerate(policy):\n",
    "        actions = policy[index].get_choices()\n",
    "        action_indices = [get_index_for_action(action) for action in actions]\n",
    "        corresponding_action_values = action_values[index][action_indices]\n",
    "        max_action_value = np.max(corresponding_action_values)\n",
    "        max_indices = np.where(corresponding_action_values == max_action_value)\n",
    "        corresponding_action_values = corresponding_action_values[max_indices]\n",
    "        if len(corresponding_action_values) < len(actions):\n",
    "            changed_policy = True\n",
    "            cell_policy.update(actions[max_indices])\n",
    "\n",
    "    return policy, changed_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now optimize the policy based on the converged action values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_world = GridWorld()\n",
    "iterations = 15\n",
    "discount = 0.9\n",
    "action_value_arr = np.zeros((5, 5, 4))\n",
    "diff_arr = []\n",
    "\n",
    "# initially: random policy\n",
    "policy = np.array([[CellPolicy([Direction.RIGHT, Direction.LEFT, Direction.UP, Direction.DOWN]) for _ in range(5)] for _ in range(5)])\n",
    "\n",
    "def get_action_value(policy, action, state_index, action_value_arr):\n",
    "    new_value = 0\n",
    "    reward, next_state = grid_world.move(state_index, action)\n",
    "    possible_actions = policy[next_state].get_choices()\n",
    "    for action in possible_actions:\n",
    "        idx = get_index_for_action(action);\n",
    "        action_value_arr_index = next_state + (idx,)\n",
    "        new_value += 1 / len(possible_actions) * (action_value_arr[action_value_arr_index])\n",
    "    return reward + discount * new_value\n",
    "\n",
    "changed_policy = True\n",
    "while changed_policy:\n",
    "    \n",
    "    diff = np.inf\n",
    "    action_value_arr = np.zeros((5, 5, 4))\n",
    "    \n",
    "    # let the action values converge\n",
    "    while(diff > 0.001):\n",
    "        diff = 0\n",
    "        for index, _ in np.ndenumerate(grid_world.grid):\n",
    "            new_value = 0\n",
    "            possible_actions = policy[index].get_choices()\n",
    "            for action in possible_actions:\n",
    "                action_idx = get_index_for_action(action)\n",
    "                new_value = get_action_value(policy, action, index, action_value_arr)\n",
    "                \n",
    "                new_diff = abs(new_value - action_value_arr[index + (action_idx,)])\n",
    "                diff = max(diff, new_diff)\n",
    "                \n",
    "                action_value_arr[index + (action_idx,)] = new_value\n",
    "        \n",
    "\n",
    "    policy, changed_policy = update_policy_av(policy, action_value_arr)\n",
    "\n",
    "print_policy(policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_reinforcement",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
