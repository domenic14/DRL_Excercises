{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 2.1 - Value-Funktion für eine Grid-World Umgebung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Klassendefinition für das Grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Direction(Enum):\n",
    "    RIGHT = (0, 1)\n",
    "    LEFT = (0, -1)\n",
    "    UP = (-1, 0)\n",
    "    DOWN = (1, 0)\n",
    "\n",
    "\n",
    "class GridWorld:\n",
    "\n",
    "    def __init__(self):\n",
    "        state_dtype = np.dtype([('key', 'U2'), ('state_value', 'f4')])\n",
    "        self.grid = np.array([[(None, 0) for _ in range(5)]\n",
    "                             for _ in range(5)], dtype=state_dtype)\n",
    "        self.grid[0, 1]['key'] = 'A'\n",
    "        self.grid[0, 3]['key'] = 'B'\n",
    "        self.grid[4, 1]['key'] = 'A_'\n",
    "        self.grid[2, 3]['key'] = 'B_'\n",
    "\n",
    "    def move(self, position, direction: Direction):\n",
    "        # basically (pos[0]+dir[0], pos[1]+dir[1])\n",
    "        new_position = tuple(sum(x) for x in zip(position, direction.value))\n",
    "\n",
    "        if self.grid[position]['key'] == 'A':\n",
    "            new_position = (4,1)\n",
    "            return 10, new_position\n",
    "\n",
    "        elif self.grid[position]['key'] == 'B':\n",
    "            new_position = (2,3)\n",
    "            return 5, new_position\n",
    "        \n",
    "        if new_position[0] < 0 or new_position[1] < 0 or new_position[0] > 4 or new_position[1] > 4:\n",
    "            return -1, position\n",
    "        else:\n",
    "            return 0, new_position\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) - State Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterativer Algorithmus zur Bestimmung der State Values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_world = GridWorld()\n",
    "iterations = 15\n",
    "discount = 0.9\n",
    "diff_arr = []\n",
    "\n",
    "def print_status(iteration):\n",
    "    print('--- STATUS ---')\n",
    "    print('Grid:')\n",
    "    #print(np.array([[tup['state_value'] for tup in row] for row in grid_world.grid]))\n",
    "    for row in grid_world.grid:\n",
    "        print('- ' * len(grid_world.grid[0]) * 5)\n",
    "        print('|', end='')\n",
    "        for cell in row:\n",
    "            cell_value = cell['state_value']\n",
    "\n",
    "            # create equal spacing for printing    \n",
    "            value_prefix = ' ' if cell_value > 0 else ''\n",
    "            value_prefix = '' if cell_value >= 10 else value_prefix\n",
    "            cell_value = round(cell_value, 4)\n",
    "            print(f' {value_prefix}{cell_value:.4f} |', end='')\n",
    "        \n",
    "        print()\n",
    "    print('- ' * len(grid_world.grid[0]) * 5)\n",
    "    print('Greatest value change compared to last iteration:')\n",
    "    print(diff_arr[iteration])\n",
    "    print('--- END STATUS ---')\n",
    "    print()\n",
    "\n",
    "iterations_to_print = [0, 1, 5, iterations - 1]\n",
    "for i in range(iterations):\n",
    "    diff = 0\n",
    "    for index, value in np.ndenumerate(grid_world.grid):\n",
    "        new_value = 0\n",
    "        for action in list(Direction):\n",
    "            reward, next_state = grid_world.move(index, action)\n",
    "            new_value += (1 / 4) * (reward + discount *\n",
    "                                    grid_world.grid[next_state]['state_value'])\n",
    "        \n",
    "        new_diff = abs(new_value - value['state_value'])\n",
    "        diff = max(diff, new_diff)\n",
    "\n",
    "        value['state_value'] = new_value\n",
    "\n",
    "    diff_arr.append(diff)\n",
    "    if i in iterations_to_print:\n",
    "        print_status(i)\n",
    "\n",
    "\n",
    "plt.plot(diff_arr)\n",
    "plt.xlabel('# Iterations')\n",
    "plt.ylabel('Greatest difference to last iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) - Action Values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Betrachtung der Action-Values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_world = GridWorld()\n",
    "iterations = 15\n",
    "discount = 0.9\n",
    "action_value_arr = np.zeros((5, 5, 4))\n",
    "diff_arr = []\n",
    "\n",
    "def action_value(action, state_index, action_value_arr):\n",
    "    new_value = 0\n",
    "    reward, next_state = grid_world.move(state_index, action)\n",
    "    for i in range(len(Direction)):\n",
    "        action_value_arr_index = next_state + (i,)\n",
    "        new_value += 1 / 4 * (action_value_arr[action_value_arr_index])\n",
    "    return reward + discount * new_value\n",
    "\n",
    "for i in range(iterations):\n",
    "    diff = 0\n",
    "    for index, _ in np.ndenumerate(grid_world.grid):\n",
    "        new_value = 0\n",
    "        for j, action in enumerate(list(Direction)):\n",
    "            new_value = action_value(action, index, action_value_arr)\n",
    "            \n",
    "            new_diff = abs(new_value - action_value_arr[index + (j,)])\n",
    "            diff = max(diff, new_diff)\n",
    "            \n",
    "            action_value_arr[index + (j,)] = new_value\n",
    "    \n",
    "    diff_arr.append(diff)\n",
    "\n",
    "plt.plot(diff_arr)\n",
    "plt.xlabel('# Iterations')\n",
    "plt.ylabel('Greatest difference to last iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the final action values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_grid(grid):\n",
    "    for row in grid:\n",
    "        print('- ' * len(grid[0]) * 5)\n",
    "        print('|', end='')\n",
    "        for cell in row:\n",
    "            # create equal spacing for printing    \n",
    "            value_prefix = ' ' if cell > 0 else ''\n",
    "            value_prefix = '' if cell >= 10 else value_prefix\n",
    "            print(f' {value_prefix}{cell:.4f} |', end='')\n",
    "        \n",
    "        print()\n",
    "    print('- ' * len(grid[0]) * 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Final action values:', end='\\n\\n')\n",
    "\n",
    "for index, _ in enumerate(list(Direction)):\n",
    "    print(list(Direction)[index].name)\n",
    "    print_grid(action_value_arr[:,:,index])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 2.2 - OpenAI Gym: Implementieren der Grid-World:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.envs.registration import register"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) - Erstellung der Grid-World Umgebung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erstellen der Umgebung, sie erbt von gym.Env:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorldGym(gym.Env):\n",
    "\n",
    "    metadata = {\"render_modes\": [\"ascii\"], \"render_fps\": 30}\n",
    "\n",
    "    def __init__(self):\n",
    "        super(GridWorldGym, self).__init__()\n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "        \"\"\" \n",
    "        0 == UP\n",
    "        1 == DOWN\n",
    "        2 == LEFT\n",
    "        3 == RIGHT \n",
    "        \"\"\"\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=np.array([0, 0]), high=np.array([4, 4]), dtype=int)\n",
    "        self.special_states = {\n",
    "            'A': (0, 1),\n",
    "            'B': (0, 3),\n",
    "            'A_': (4, 1),\n",
    "            'B_': (2, 3)\n",
    "        }\n",
    "        self.state = np.zeros((2), dtype=int)\n",
    "        self.render_mode = 'ascii'\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)  # Ruft die Basis-Reset-Funktion mit seed auf\n",
    "\n",
    "        self.done = False\n",
    "        self.state = np.zeros((2), dtype=int)\n",
    "        return np.array(self.state), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        new_state = None\n",
    "        reward = 0\n",
    "        \n",
    "        if action == 0:\n",
    "            new_state = self.state + (-1, 0)\n",
    "        elif action == 1:\n",
    "            new_state = self.state + (1, 0)\n",
    "        elif action == 2:\n",
    "            new_state = self.state + (0, -1)\n",
    "        elif action == 3:\n",
    "            new_state = self.state + (0, 1)\n",
    "        else:\n",
    "            raise ValueError('Fehler')\n",
    "\n",
    "        if new_state[0] < 0 or new_state[1] < 0 or new_state[0] > 4 or new_state[1] > 4:\n",
    "            reward = -1\n",
    "        elif np.array_equal(self.state, self.special_states[\"A\"]):\n",
    "            reward = 10\n",
    "            self.state = np.array(self.special_states[\"A_\"])\n",
    "            done = True\n",
    "        elif np.array_equal(self.state, self.special_states[\"B\"]):\n",
    "            reward = 5\n",
    "            self.state = np.array(self.special_states[\"B_\"])\n",
    "            done = True\n",
    "        else:\n",
    "            self.state = new_state\n",
    "\n",
    "        return self.state, reward, done, False, {}\n",
    "    \n",
    "    def render(self):\n",
    "        \n",
    "        if self.render_mode != 'ascii':\n",
    "            raise NotImplementedError\n",
    "\n",
    "        render_str = ''\n",
    "        for row_idx in range(5):\n",
    "            render_str += '- ' * 16\n",
    "            render_str += '\\n'\n",
    "            render_str += '|'\n",
    "            for col_idx in range(5):\n",
    "                render_str += ' '\n",
    "                \n",
    "                if np.array_equal(self.state, [row_idx, col_idx]):\n",
    "                    render_str += 'X'\n",
    "                else:\n",
    "                    render_str += ' '\n",
    "\n",
    "                if np.array_equal(self.special_states[\"A\"], [row_idx, col_idx]):\n",
    "                    render_str += 'A '\n",
    "                elif np.array_equal(self.special_states[\"A_\"], [row_idx, col_idx]):\n",
    "                    render_str += 'A_'\n",
    "                elif np.array_equal(self.special_states[\"B\"], [row_idx, col_idx]):\n",
    "                    render_str += 'B '\n",
    "                elif np.array_equal(self.special_states[\"B_\"], [row_idx, col_idx]):\n",
    "                    render_str += 'B_'\n",
    "                else:\n",
    "                    render_str += '  '\n",
    "                \n",
    "                render_str += ' |'\n",
    "            render_str += '\\n'\n",
    "        render_str += '- ' * 16\n",
    "        render_str += '\\nLegend: X = current state, A, B, A_, B_ = special states'\n",
    "\n",
    "        return render_str\n",
    "\n",
    "    \n",
    "    def close(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Registrieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "register(\n",
    "    id='GridWorldGym-v0',\n",
    "    entry_point=GridWorldGym,\n",
    "    max_episode_steps=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) - Testen der Grid-World Umgebung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"GridWorldGym-v0\")\n",
    "\n",
    "\n",
    "total_reward = 0    # Gesamtbelohnung initialisieren\n",
    "done = False\n",
    "\n",
    "# speichert die Umgebungs-Visualisierung als Ascii-Strings\n",
    "visualization = []\n",
    "\n",
    "# Führe mehrere Schritte aus\n",
    "while len(visualization) < 4:\n",
    "    obs, info = env.reset()  # Setze die Umgebung zurück\n",
    "    print(obs)\n",
    "\n",
    "    while not done:\n",
    "        action = env.action_space.sample()  # Wähle eine zufällige Aktion\n",
    "        obs, reward, done, truncated, info = env.step(action)  # Führe die Aktion aus\n",
    "        total_reward += reward  # Update die Gesamtbelohnung\n",
    "        \n",
    "        if len(visualization) < 4:\n",
    "            visualization.append(env.render())\n",
    "\n",
    "        print(f\"Aktion: {action}, Neue Beobachtung: {obs}, Belohnung: {reward}, Done {done}\")\n",
    "\n",
    "print(f\"Gesamtbelohnung nach der Episode: {total_reward}\")\n",
    "\n",
    "# Visualisierung von 4 aufeinanderfolgenden Schritten (mögliches Episodenende dazwischen)\n",
    "for vis_index in range(len(visualization)):\n",
    "    vis_string = visualization[vis_index]\n",
    "    print(f'Step {vis_index}:')\n",
    "    print(vis_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 2.3 - CartPole Umgebung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Einfache Visualisierung mit zufällig ausgewählten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "\n",
    "visualization_iterations = 10\n",
    "\n",
    "for _ in range(visualization_iterations):\n",
    "    observation, info = env.reset()\n",
    "    \n",
    "    episode_over = False\n",
    "    \n",
    "    while not episode_over:\n",
    "        action = env.action_space.sample()  # random right or left\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        episode_over = terminated or truncated\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) - Implementierung eines intuitiven Lösungsansatzes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = 100\n",
    "num_episodes = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy 1: Wenn der Pole nach links geneigt ist, fahre nach links. Wenn der Pole nach rechts geneigt ist, fahre nach rechts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, info = env.reset()\n",
    "\n",
    "rewards = np.zeros(num_episodes)\n",
    "\n",
    "for _ in range(runs):\n",
    "    \n",
    "    for episode_index in range(num_episodes):\n",
    "\n",
    "        observation, info = env.reset()\n",
    "\n",
    "        episode_over = False\n",
    "        episode_reward = 0\n",
    "\n",
    "        while not episode_over:\n",
    "            action = 1  # push cart to the right\n",
    "            \n",
    "            # if the pole is leaning to the left, push the cart to the left\n",
    "            if observation[2] < 0:\n",
    "                action = 0\n",
    "            \n",
    "            observation, reward, terminated, truncated, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "\n",
    "            episode_over = terminated or truncated\n",
    "        \n",
    "        rewards[episode_index] += episode_reward\n",
    "\n",
    "rewards_p1 = np.divide(rewards, runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy 2: Wenn der Pole nach links geneigt ist aber eine positive Winkelgeschwindigkeit hat, fahre wieder nach rechts. Analog für Neigung nach rechts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, info = env.reset()\n",
    "\n",
    "rewards = np.zeros(num_episodes)\n",
    "\n",
    "for _ in range(runs):\n",
    "    \n",
    "    for episode_index in range(num_episodes):\n",
    "\n",
    "        observation, info = env.reset()\n",
    "\n",
    "        episode_over = False\n",
    "        episode_reward = 0\n",
    "\n",
    "        while not episode_over:\n",
    "            action = 1  # push cart to the right\n",
    "            \n",
    "            # if the pole is leaning to the left, push the cart to the left\n",
    "            falling_left = observation[3] < 0\n",
    "            if falling_left:\n",
    "                action = 0\n",
    "            \n",
    "            observation, reward, terminated, truncated, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "\n",
    "            episode_over = terminated or truncated\n",
    "        \n",
    "        rewards[episode_index] += episode_reward\n",
    "\n",
    "rewards_p2 = np.divide(rewards, runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy 3: Zusätzliche Vermeidung des \"Herausfahrens\" aus der Umgebung:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, info = env.reset()\n",
    "\n",
    "rewards = np.zeros(num_episodes)\n",
    "\n",
    "for _ in range(runs):\n",
    "\n",
    "    for episode_index in range(num_episodes):\n",
    "\n",
    "        observation, info = env.reset()\n",
    "\n",
    "        episode_over = False\n",
    "        episode_reward = 0\n",
    "\n",
    "        while not episode_over:\n",
    "            action = 1  # push cart to the right\n",
    "            \n",
    "            # if the pole is falling to the left, push the cart to the left\n",
    "            # (falling meaning the angular velocity is negative)\n",
    "            falling_left = observation[3] < 0\n",
    "            if falling_left:\n",
    "                action = 0\n",
    "            \n",
    "            # do not drive out of the environment\n",
    "            cart_position = observation[0]\n",
    "            if cart_position <= -2.2:\n",
    "                action = 1\n",
    "            elif cart_position >= 2.2:\n",
    "                action = 0\n",
    "\n",
    "            observation, reward, terminated, truncated, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "\n",
    "            episode_over = terminated or truncated\n",
    "\n",
    "        rewards[episode_index] += episode_reward \n",
    "\n",
    "rewards_p3 = np.divide(rewards, runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment schließen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auswerten der Policy-Ergebnisse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, num_episodes, num_episodes)\n",
    "\n",
    "plt.plot(x, rewards_p1, label = 'Policy 1')\n",
    "plt.plot(x, rewards_p2, label = 'Policy 2')\n",
    "plt.plot(x, rewards_p3, label = 'Policy 3')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Reward per Episode')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) - RL-Ansatz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ObservationWrapper erstellen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DiscreteObservationWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(DiscreteObservationWrapper, self).__init__(env)\n",
    "        self.pole_pos_bins = np.array([-np.inf, 0.0, np.inf])\n",
    "        self.pole_vel_bins = np.array([-np.inf, 0.0, np.inf])\n",
    "        self.num_states = 4\n",
    "        \n",
    "        self.min_state_idx = self.get_state_index(1, 1) # since the values are always greater than negative infinity, they are always in category 1 and never in zero. Thats why category tuple (1, 1) leads to the lowest individual state index\n",
    "\n",
    "    def get_state_index(self, pole_pos_category, pole_vel_category):\n",
    "        return 2 * pole_pos_category + pole_vel_category\n",
    "\n",
    "    def observation(self, observation):\n",
    "        _, _, pole_pos, pole_vel = observation\n",
    "        \n",
    "        pole_pos_category = np.digitize(pole_pos, self.pole_pos_bins)\n",
    "        pole_vel_category = np.digitize(pole_vel, self.pole_vel_bins)\n",
    "\n",
    "        # basically binary \"or\" operation, each category gets its own bit\n",
    "        state_index = self.get_state_index(pole_pos_category, pole_vel_category)\n",
    "\n",
    "        return state_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Den Banditen durch eine Klasse modellieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class StateBandit():\n",
    "    def __init__(self, epsilon, initial_Q = 0.0):\n",
    "        self.epsilon = epsilon\n",
    "        self.Q = np.array([initial_Q, initial_Q])\n",
    "        self.N = np.array([0, 0])\n",
    "        self.G = 0\n",
    "        self.reward_per_action = []\n",
    "\n",
    "    def get_action(self):\n",
    "        action = np.argmax(self.Q)\n",
    "\n",
    "        # decide on exploit vs explore\n",
    "        explore = random.random() <= self.epsilon\n",
    "        if explore:\n",
    "            action = int(random.random() >= 0.5)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def update_metrics(self, action, reward):\n",
    "        self.reward_per_action.append(reward)\n",
    "        self.G += reward\n",
    "        self.N[action] += 1\n",
    "        self.Q[action] = self.Q[action] + ((reward - self.Q[action]) / self.N[action])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für jeden State einen Banditen erstellen und die Simulation laufen lassen, um die Banditen anzulernen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import stdout # flushing stdout for progress printing\n",
    "\n",
    "# create env with wrapper\n",
    "env = gym.make('CartPole-v1')\n",
    "wrapped_env = DiscreteObservationWrapper(env)\n",
    "\n",
    "# setting up the bandits\n",
    "epsilon_value = 0.1\n",
    "initial_Q = 1.0\n",
    "bandits = [StateBandit(epsilon_value, initial_Q) for _ in range(wrapped_env.num_states)]\n",
    "\n",
    "# training parameters\n",
    "simulation_steps = 40_000\n",
    "\n",
    "# reset wrapped env\n",
    "state_index, _ = wrapped_env.reset()\n",
    "episode_over = False\n",
    "\n",
    "# training loop\n",
    "progress = -1\n",
    "sim_step = 0\n",
    "while sim_step < simulation_steps:\n",
    "    \n",
    "    new_progress = int(((sim_step + 1) * 100) / simulation_steps)\n",
    "    if new_progress > progress:\n",
    "        progress = new_progress\n",
    "        print(f'\\rProgress: {progress} %', end='')\n",
    "        stdout.flush()\n",
    "\n",
    "    bandit_index = state_index - wrapped_env.min_state_idx\n",
    "    action = bandits[bandit_index].get_action()\n",
    "    state_index, reward, terminated, truncated, _ = wrapped_env.step(action)\n",
    "\n",
    "    reward = -10 if terminated else reward\n",
    "\n",
    "    bandits[bandit_index].update_metrics(action, reward)\n",
    "\n",
    "    episode_over = terminated or truncated\n",
    "    \n",
    "    if episode_over:\n",
    "        sim_step += 1\n",
    "        episode_over = False\n",
    "        state_index, _ = wrapped_env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotten des Lernprozesses, Nutzung eines \"Dictionarys\", Banditen-Index auf die Zustandsbeschreibung zu mappen. Die Verwendung eines vollwertigen Python-Dicts ist hier nicht notwendig, da die Banditen von Index 0 aufsteigend geplottet werden. Das heißt, man kann die Zustandsbeschreibungen auch chronologisch ablegen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_index_description = ['Negative angle, negative velocity', 'Negative angle, positive velocity', 'Positive angle, Negative velocity', 'Positive angle, Positive velocity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(len(bandits), figsize=(15, 15))\n",
    "for bandit_index in range(len(bandits)):\n",
    "    bandit = bandits[bandit_index]\n",
    "    ax[bandit_index].plot(bandit.reward_per_action, label = f'State {bandit_index} ({state_index_description[bandit_index]})')\n",
    "    ax[bandit_index].set_xlabel('# Actions')\n",
    "    ax[bandit_index].set_ylabel('Reward per Action')\n",
    "    ax[bandit_index].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testen, wie gut die Banditen sind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 200\n",
    "runs = 100\n",
    "\n",
    "state_index, info = wrapped_env.reset()\n",
    "\n",
    "rewards = np.zeros(num_episodes)\n",
    "\n",
    "for _ in range(runs):\n",
    "    \n",
    "    for episode_index in range(num_episodes):\n",
    "\n",
    "        state_index, info = wrapped_env.reset()\n",
    "\n",
    "        episode_over = False\n",
    "        episode_reward = 0\n",
    "\n",
    "        while not episode_over:\n",
    "            bandit_index = state_index - wrapped_env.min_state_idx\n",
    "            action = bandits[bandit_index].get_action()\n",
    "\n",
    "            state_index, reward, terminated, truncated, info = wrapped_env.step(action)\n",
    "            episode_reward += reward\n",
    "\n",
    "            episode_over = terminated or truncated\n",
    "        \n",
    "        rewards[episode_index] += episode_reward\n",
    "\n",
    "rewards_RL = np.divide(rewards, runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Darstellung der Testergebnisse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, num_episodes, num_episodes)\n",
    "\n",
    "plt.plot(x, rewards_RL, label = 'Reinforcement Learning')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Reward per Episode')\n",
    "plt.ylim(0, 240)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schließen der Environments (Da diese noch für den Test verwendet wurden, kann man sie nicht direkt nach dem Training schließen):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_env.close()\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_reinforcement",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
